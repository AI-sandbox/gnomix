{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Benchmarking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(bases, smoothers, chm, gen_expts, datasets, model_root):\n",
    "\n",
    "    for data_path in datasets:\n",
    "\n",
    "        # data_path must end with generated_data\n",
    "        data_path = data_path + \"generated_data/\"\n",
    "\n",
    "        for gen_name in gen_expts:\n",
    "            \n",
    "            data_specs = \"/\".join(data_path.split(\"/\")[-5:-2]) + \"/chm{}/\".format(chm)\n",
    "            model_path = model_root + data_specs\n",
    "            if not os.path.exists(model_path):\n",
    "                os.makedirs(model_path)\n",
    "                \n",
    "            snp = data_path.split(\"/\")[5]\n",
    "            if snp == \"full\":\n",
    "                M = 1000\n",
    "            elif snp== \"ukb\":\n",
    "                M = 30\n",
    "\n",
    "            gens = gen_expts[gen_name]\n",
    "\n",
    "            print(\"-\"*100)\n",
    "            print(\"Experiment details\")\n",
    "            print(\"Dataset used: {}\".format(data_path))\n",
    "            print(\"Generations used: {}\".format(gens))\n",
    "            print(\"Models will be stored at: {}\".format(model_path))\n",
    "            \n",
    "            bm_train(bases, smoothers, model_path=model_path, data_path=data_path, gens=gens, chm=chm, M=M)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_benchmark = [\n",
    "# Full Genome\n",
    "# \"/home/arvindsk/xgmix_expts/benchmark_data/full/bal_admix/latino/\",\n",
    "# \"/home/arvindsk/xgmix_expts/benchmark_data/full/bal_admix/five/\",\n",
    "# \"/home/arvindsk/xgmix_expts/benchmark_data/full/bal_admix/seven/\",\n",
    "\n",
    "# Array data\n",
    "# \"/home/arvindsk/xgmix_expts/benchmark_data/ukb/bal_admix/latino/\",\n",
    "# \"/home/arvindsk/xgmix_expts/benchmark_data/ukb/bal_admix/five/\",\n",
    "\"/home/arvindsk/xgmix_expts/benchmark_data/ukb/bal_admix/seven/\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = '/home/wknd37/gnomix/benchmark/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Experiment details\n",
      "Dataset used: /home/arvindsk/xgmix_expts/benchmark_data/ukb/bal_admix/seven/generated_data/\n",
      "Generations used: [0, 2, 4, 8, 16, 24, 32, 48, 64, 72, 100]\n",
      "Models will be stored at: /home/wknd37/gnomix/benchmark/models/ukb/bal_admix/seven/chm20/\n",
      "Reading data...\n",
      "BASE: logreg\n",
      "Training base\n",
      "Windows done: 532/532\n",
      "Re-training base\n",
      "Windows done: 532/532\n",
      "BASE: lgb\n",
      "Training base\n",
      "Windows done: 63/532"
     ]
    }
   ],
   "source": [
    "bases = [\"logreg\", \"lgb\", \"randomstringkernel\", \"stringkernel\"]\n",
    "smoothers = [\"crf\", \"xgb\", \"cnn\"]\n",
    "\n",
    "chm = 20\n",
    "gen_expts = {\n",
    "    \"all\" : [0,2,4,8,16,24,32,48,64,72,100]\n",
    "}\n",
    "train(bases, smoothers, chm, gen_expts, datasets=datasets_benchmark, model_root=model_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Benchmarking import get_data\n",
    "\n",
    "def evaluate(bases, smoothers, chm, val_gens, data_path_fmt, model_path_fmt):\n",
    "\n",
    "#     for set_name in [\"full\", \"ukb\"]:\n",
    "    for set_name in [\"ukb\"]:\n",
    "        M = 30 if set_name == \"ukb\" else 1000\n",
    "\n",
    "#         for pop_name in [\"latino\",\"five\",\"seven\"]:\n",
    "        for pop_name in [\"latino\"]:\n",
    "\n",
    "            data_path = data_path_fmt.format(chm,set_name,pop_name)\n",
    "            model_root = model_path_fmt.format(chm,set_name,pop_name)\n",
    "\n",
    "            print(\"Getting traintime data for model meta data\")\n",
    "            train_data_root = \"/home/arvindsk/xgmix_expts/benchmark_data/{0}/{1}/{2}/generated_data/\"\n",
    "            train_data_path = train_data_root.format(set_name,\"bal_admix\",pop_name)\n",
    "            _, meta, _ = get_data(train_data_path, M=M, gens=[0,2], chm=chm, verbose=False)\n",
    "            \n",
    "            print(\"Getting validation set that was separately generated\")\n",
    "            print(\"Val generations: \",val_gens)\n",
    "            val_paths = [data_path + \"/chm{}/simulation_output/val/gen_\".format(chm) + str(gen) + \"/\" for gen in val_gens] \n",
    "            X_val_files = [p + \"mat_vcf_2d.npy\" for p in val_paths]\n",
    "            labels_val_files = [p + \"mat_map.npy\" for p in val_paths]\n",
    "            train_val_files = [X_val_files, labels_val_files]\n",
    "            X_val_raw, labels_val_raw = [load_np_data(f) for f in train_val_files]\n",
    "            X_val, labels_window_val = data_process(X_val_raw, labels_val_raw, M, 0)\n",
    "            X_v = np.array(X_val).astype(\"int8\")\n",
    "            y_v = np.array(labels_window_val).astype(\"int16\")\n",
    "            y_snp = labels_val_raw.copy() # glue the old train data with the new val data\n",
    "\n",
    "            for b in bases:\n",
    "                for s in smoothers:\n",
    "\n",
    "                    base_path = model_root + \"base_models/\" + b + \"_retrained.pkl\"\n",
    "                    smooth_path = model_root + \"smoothers/\" + b + \"_\" + s + \".pkl\"\n",
    "                    metric_path = model_root + b + \"_\" + s + \".benchmark\"\n",
    "                    \n",
    "                    print(\"Base:\", base_path)\n",
    "                    print(\"Smooth:\", smooth_path)\n",
    "\n",
    "                    model = Gnomix(C=meta[\"C\"], M=meta[\"M\"], A=meta[\"A\"])\n",
    "                    model.base = pickle.load(open(base_path,\"rb\"))\n",
    "                    model.smooth = pickle.load(open(smooth_path,\"rb\"))\n",
    "\n",
    "                    metrics = bm_eval(model, val_data=(X_v, y_v), gens=val_gens, y_snp=y_snp,\n",
    "                                      base_smooth_paths=[base_path, smooth_path])\n",
    "                    save_dict(metrics,metric_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of chm, set_name, pop_name\n",
    "model_path_fmt = \"/home/wknd37/gnomix/benchmark/models/{1}/bal_admix/{2}/chm{0}/\"\n",
    "data_path_fmt = \"/home/arvindsk/xgmix_expts/benchmark_data/val/chm{0}/{1}/{2}/generated_data/\"\n",
    "val_gens = [2,4,8,12,16,20,30,40,50,60,70,80,90,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting traintime data for model meta data\n",
      "Getting validation set that was separately generated\n",
      "Val generations:  [2, 4, 8, 12, 16, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "Base: /home/wknd37/gnomix/benchmark/models/ukb/bal_admix/latino/chm20/base_models/lgb_retrained.pkl\n",
      "Smooth: /home/wknd37/gnomix/benchmark/models/ukb/bal_admix/latino/chm20/smoothers/lgb_crf.pkl\n",
      "{'base_val_acc': 83.98, 'base_val_acc_bal': 80.27, 'base_val_log_loss': 0.5, 'smooth_val_acc': 96.07, 'smooth_val_acc_bal': 94.51, 'smooth_val_log_loss': 0.12, 'model_total_size_mb': 42.4, 'gen_performance': {'gens': [2, 4, 8, 12, 16, 20, 30, 40, 50, 60, 70, 80, 90, 100], 'accs': [98.53, 98.97, 98.71, 98.32, 97.96, 97.39, 97.09, 96.61, 94.76, 95.04, 93.74, 93.54, 92.48, 91.79], 'accs_snp_lvl': [98.52, 98.96, 98.66, 98.23, 97.87, 97.31, 96.98, 96.38, 94.51, 94.6, 93.32, 93.09, 92.02, 91.31]}, 'val_acc_snp_lvl': 95.84}\n"
     ]
    }
   ],
   "source": [
    "# evaluate(bases, smoothers, chm, gen_expts, datasets=datasets_benchmark, model_root=model_root)\n",
    "evaluate([\"lgb\"], [\"crf\"], chm, val_gens=val_gens, data_path_fmt=data_path_fmt, model_path_fmt=model_path_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics_benchmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9b0355302a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msmooth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"xgb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"crf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnv\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bal_admix\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mmetric_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{}_{}_{}.benchmark\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mskip_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"gen_performance\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics_benchmark' is not defined"
     ]
    }
   ],
   "source": [
    "chm = 20\n",
    "expt = [\"model\",\"base\",\"smooth\",\"pop_name\",\"bal\",\"snp\"]\n",
    "tuples = []\n",
    "# for set_name in [\"full\",\"ukb\"]:\n",
    "for set_name in [\"ukb\"]:\n",
    "#     for pop_name in [\"latino\",\"five\",\"seven\"]:\n",
    "    for pop_name in [\"latino\"]:\n",
    "        data_path = data_path_fmt.format(chm,set_name,pop_name)\n",
    "        for base in [\"lgb\", \"logreg\", \"randomstringkernel\", \"stringkernel\"]:\n",
    "            for smooth in [\"xgb\", \"crf\", \"cnv\"]:\n",
    "                for bal in [\"bal_admix\"]:\n",
    "                    model_root = model_path_fmt.format(chm,set_name,pop_name)\n",
    "                    metric_path = model_root + b + \"_\" + s + \".benchmark\"\n",
    "                    metrics = load_dict(metric_path)\n",
    "                    skip_metrics = [\"gen_performance\"]\n",
    "                    metric_names = [mkey for mkey in list(metrics.keys()) if mkey not in skip_metrics]\n",
    "                    metrics = tuple([metrics[m] for m in metric_names])\n",
    "                    tuples += [(base+\"_\"+smooth,base,smooth,pop_name,bal,set_name)+metrics]\n",
    "                    \n",
    "print(metric_names)\n",
    "DF_us = pd.DataFrame(tuples, columns=expt+metric_names)\n",
    "DF_us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
