{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arvindsk/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from Benchmarking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"arvindsk\"\n",
    "# user = \"wknd37\"\n",
    "\n",
    "datasets_benchmark = [\n",
    "    \n",
    "# # full set of full-unbal_unbal\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/unbal_unbal/latino/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/unbal_unbal/five/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/unbal_unbal/seven/\",\n",
    "\n",
    "# # full set of full-bal_admix\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_admix/latino/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_admix/five/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_admix/seven/\",\n",
    "\n",
    "# UKB\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/unbal_unbal/latino/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/unbal_unbal/five/\",\n",
    "\"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/unbal_unbal/seven/\",\n",
    "    \n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_admix/latino/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_admix/five/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_admix/seven/\",\n",
    "\n",
    "# # full-bal_unbal only for seven and latino - this way we complete a latino sweep to compare different balances\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_unbal/seven/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_unbal/latino/\",\n",
    "# # full-bal_bal only for latino - seems like a nice expt\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/full/bal_bal/latino/\",\n",
    "\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_unbal/seven/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_unbal/latino/\",\n",
    "# \"/home/\" + user + \"/xgmix_expts/benchmark_data/ukb/bal_bal/latino/\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bm(base, smooth, chm, gen_expts, models_exist=False, local_metric=None):\n",
    "\n",
    "    for data_path in datasets_benchmark:\n",
    "\n",
    "        # data_path must end with generated_data\n",
    "        data_path = data_path + \"generated_data/\"\n",
    "\n",
    "        for gen_name in gen_expts:\n",
    "\n",
    "            bm_root = data_path + \"chm{}/{}/\".format(chm,gen_name)\n",
    "            snp, bal, dat = np.array(bm_root.split(\"/\"))[-7:-4]\n",
    "            \n",
    "            if snp == \"full\":\n",
    "                W = 1000\n",
    "            elif snp== \"ukb\":\n",
    "                W = 30\n",
    "            \n",
    "            metric_path = bm_root + \"test_sk_benchmark_results.pkl\"\n",
    "            if local_metric is not None:\n",
    "                metric_path = metric_path.replace(\"arvindsk\", local_metric)\n",
    "\n",
    "            gens = gen_expts[gen_name]\n",
    "\n",
    "            print(\"-\"*100)\n",
    "            print(\"Experiment details\")\n",
    "            print(\"Dataset used: {}\".format(data_path))\n",
    "            print(\"Generations used: {}\".format(gens))\n",
    "            print(\"Metrics will come up at: {}\".format(metric_path))\n",
    "\n",
    "            metrics = bm_train(base, smooth, \n",
    "                               bm_root, data_path, \n",
    "                               gens, chm=chm,\n",
    "                               W=W, \n",
    "                               load_base=True, load_smooth=True, \n",
    "                               eval=False,\n",
    "                               models_exist=models_exist,\n",
    "                               verbose=True,\n",
    "                               only_founders=True,# FOR STRING KERNEL\n",
    "                               quick_eval=True\n",
    "                              )\n",
    "\n",
    "            save_dict(metrics, metric_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Experiment details\n",
      "Dataset used: /home/arvindsk/xgmix_expts/benchmark_data/ukb/unbal_unbal/seven/generated_data/\n",
      "Generations used: [0, 2, 4, 8, 16, 24, 32, 48, 64, 72, 100]\n",
      "Metrics will come up at: /home/arvindsk/xgmix_expts/benchmark_data/ukb/unbal_unbal/seven/generated_data/chm20/all/test_sk_benchmark_results.pkl\n",
      "Reading data...\n",
      "BASE: svm_random_string_kernel\n",
      "/home/arvindsk/xgmix_expts/benchmark_data/ukb/unbal_unbal/seven/generated_data/chm20/all/base_models/svm_random_string_kernel.pkl\n",
      "SMOOTH: xgb\n",
      "/home/arvindsk/xgmix_expts/benchmark_data/ukb/unbal_unbal/seven/generated_data/chm20/all/models/svm_random_string_kernel_xgb.pkl\n",
      "Quick Evaluation...\n"
     ]
    }
   ],
   "source": [
    "base = [\"svm_random_string_kernel\"]\n",
    "smooth = [\"xgb\"]\n",
    "\n",
    "# here we run 3 base, 3 smooth, 2 datasets, 3 populations, 2 balance\n",
    "\n",
    "chm = 20\n",
    "gen_expts = {\n",
    "    \"all\" : [0,2,4,8,16,24,32,48,64,72,100]\n",
    "}\n",
    "run_bm(base, smooth, chm, gen_expts, models_exist=False, local_metric=\"arvindsk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base = [\"rf\",\"lgb\", \"xgb\", \"logreg\"]\n",
    "smooth = [\"xgb\", \"crf\", \"cnv\"]\n",
    "for gen_name in gen_expts:\n",
    "    for data_path in datasets_benchmark:\n",
    "        bm_root = data_path + \"generated_data/chm{}/{}/\".format(chm,gen_name)\n",
    "        print(bm_root)\n",
    "        metric_path = bm_root + \"benchmark_results.pkl\"\n",
    "        M = load_dict(metric_path)\n",
    "        accs = plt_metric(base ,smooth, M, \"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; \n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "def get_metric(base,smooth,Metrics,metric):\n",
    "    M = {}\n",
    "    for i, b in enumerate(base):\n",
    "        M[b] = {}\n",
    "        for j, s in enumerate(smooth):\n",
    "            M[b][s] = Metrics[b+\"_\"+s][metric]\n",
    "            \n",
    "    return pd.DataFrame(M)\n",
    "\n",
    "\n",
    "def plt_metric(base,smooth,Metrics,metric,vminmax=(None,None),cbar=True):\n",
    "    \n",
    "    if metric == \"gen_performance\":\n",
    "        cmap = { 0:'k', 1:'b',  2:'y', 3:'g'}\n",
    "        lmap = { 0:'-', 1:'--', 2:':', 3:'-.' }\n",
    "        v_accs = get_metric(base, smooth, Metrics, \"val_acc\")\n",
    "        for i, b in enumerate(base):\n",
    "            for j, s in enumerate(smooth):\n",
    "                m_name = b + \"_\" + s\n",
    "                data = Metrics[m_name][metric] \n",
    "                plt.plot(data[\"gens\"], data[\"accs\"], linestyle=lmap[i%len(lmap)], color=cmap[j%len(cmap)],\n",
    "                         label= m_name + \" (\" + str(v_accs[b][j]) + \"%)\")\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1, 1), loc='upper left') \n",
    "        plt.xlabel(\"Generation\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        data = get_metric(base,smooth,Metrics,metric)\n",
    "        sns.heatmap(data, annot=True, fmt=\".2f\",vmin=vminmax[0], vmax=vminmax[1], cbar=cbar)\n",
    "        plt.xlabel(\"base model\")\n",
    "        plt.ylabel(\"smoother\")\n",
    "        plt.show()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [\"rf\", \"lgb\", \"xgb\", \"logreg\"]\n",
    "smooth = [\"cnv\", \"xgb\", \"crf\"]\n",
    "\n",
    "cols = [\"base\", \"smooth\",\"data\",\"gen\",\"bal\",\"snp\",\"val_acc\"]\n",
    "ACC = [] # (b, s, data, gen, bal, ukb, acc)\n",
    "all_accs = np.zeros( (len(base), len(smooth), len(gen_expts), len(datasets_benchmark)) )\n",
    "for g, gen_name in enumerate(gen_expts):\n",
    "    for d, data_path in enumerate(datasets_benchmark):\n",
    "        bm_root = data_path + \"generated_data/chm{}/{}/\".format(chm,gen_name)\n",
    "        metric_path = bm_root + \"benchmark_results.pkl\"\n",
    "        M = load_dict(metric_path)\n",
    "        \n",
    "        v_accs = get_metric(base, smooth, M, \"val_acc\")\n",
    "        \n",
    "        dat = np.array(bm_root.split(\"/\"))[-5]\n",
    "        bal = np.array(bm_root.split(\"/\"))[-6]\n",
    "        ukb = np.array(bm_root.split(\"/\"))[-7]\n",
    "        for b in base:\n",
    "            for s in smooth:\n",
    "                ACC += [(b,s,dat,gen_name,bal,ukb,v_accs[b][s])]\n",
    "DF = pd.DataFrame(ACC, columns=cols)\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (LogReg) X (XGB, CRF) performing best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.boxplot(column=[\"val_acc\"], by=[\"snp\",\"base\", \"smooth\"], figsize=(16,8), rot=\"45\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.boxplot(column=[\"val_acc\"], by=[\"snp\", \"base\", \"smooth\"], figsize=(16,8), rot=\"45\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_best_models = DF[ \n",
    "    ~(DF.base.isin([\"rf\", \"lgb\"])) &\n",
    "    (DF.smooth != \"cnv\")\n",
    "]\n",
    "DF_best_models.boxplot(column=[\"val_acc\"], by=[\"base\", \"smooth\"], figsize=(16,8), rot=\"45\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance not affacting much but still a bit\n",
    "we want to repeat with fixed validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.boxplot(column=[\"val_acc\"], by=[\"data\", \"bal\"], figsize=(16,8), rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same pattern using only best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_best_models.boxplot(column=[\"val_acc\"], by=[\"data\", \"bal\"], figsize=(16,8), rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracies similar for high generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_lathi = DF[ \n",
    "    (DF.base != \"rf\") &\n",
    "    (DF.smooth != \"cnv\") &\n",
    "    (DF.gen==\"high\") ]\n",
    "DF_lathi.boxplot(column=[\"val_acc\"], by=[\"data\", \"snp\", \"smooth\"], figsize=(16,8), rot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/arvindsk/xgmix_expts/benchmark_data/ukb/unbal_unbal/five/\"\n",
    "base = [\"xgb\", \"logreg\"]\n",
    "smooth = [\"xgb\", \"crf\"]\n",
    "gen_name = \"high\"\n",
    "bm_root = data_path + \"generated_data/chm{}/{}/\".format(chm,gen_name)\n",
    "metric_path = bm_root + \"benchmark_results.pkl\"\n",
    "M = load_dict(metric_path)\n",
    "out = plt_metric(base ,smooth, M, \"gen_performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note! acc > 90% for gen 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
